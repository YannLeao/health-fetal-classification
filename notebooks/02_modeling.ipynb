{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b8965de",
   "metadata": {},
   "source": [
    "# Modelagem Parte 1: Árvore de Decisão e KNN \n",
    "\n",
    "**Responsável:** Clara\n",
    "\n",
    "**Objetivo:** Nesta etapa, realizamos o treinamento dos algoritmos **Decision Tree** e **K-Nearest Neighbors (KNN)** utilizando o dataset pré-processado de saúde fetal.\n",
    "\n",
    "**Metodologia:**\n",
    "* **Validação:** 10-fold Stratified Cross-Validation.\n",
    "* **Otimização:** `GridSearch` para encontrar os melhores hiperparâmetros (testando no mínimo 3 combinações).\n",
    "* **Métricas:** Acurácia, Precisão, Recall e F1-Score (*weighted*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e03f4eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados carregados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Adiciona a raiz do projeto ao PYTHONPATH\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "\n",
    "from src.modeling import run_classification_experiment\n",
    "\n",
    "data_path = os.path.join('..', 'data', 'processed', 'fetal_health_processed.csv')\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "X = df.drop('fetal_health', axis=1)\n",
    "y = df['fetal_health']\n",
    "\n",
    "print(\"Dados carregados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08de6263",
   "metadata": {},
   "source": [
    "## 1. Árvore de Decisão (Decision Tree)\n",
    "\n",
    "A árvore de decisão é um modelo interpretável que divide os dados com base em regras de decisão. Para evitar *overfitting* e encontrar a melhor generalização, testaremos os seguintes hiperparâmetros via `GridSearch`:\n",
    "\n",
    "* **Criterion:** `gini` vs `entropy` (avalia qual métrica de pureza funciona melhor na divisão dos nós).\n",
    "* **Max Depth:** `None` (sem limite), `10`, `20`, `30` (controla a profundidade máxima para limitar a complexidade da árvore).\n",
    "* **Min Samples Split:** Controla o número mínimo de amostras necessárias para dividir um nó interno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41b7b70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando o GridSearch para DecisionTree ---\n",
      "Fitting 10 folds for each of 24 candidates, totalling 240 fits\n",
      "Melhor F1 para DecisionTree: 0.9340\n",
      "Resultados salvos em: c:\\Users\\ga618\\.vscode\\Arquivos\\InteligenciaArtificial\\Projeto\\MachineLearnig-Classifiacacao\\results\\metrics\\DecisionTree_results.csv\n",
      "Melhores parâmetros DT: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 20}\n"
     ]
    }
   ],
   "source": [
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "dt_params = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 10, 20]\n",
    "}\n",
    "\n",
    "# Chama a função que está no arquivo .py\n",
    "dt_grid = run_classification_experiment(dt_model, dt_params, X, y, 'DecisionTree')\n",
    "\n",
    "print(f\"Melhores parâmetros DT: {dt_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a16660c",
   "metadata": {},
   "source": [
    "## 2. K-Nearest Neighbors (KNN)\n",
    "\n",
    "O KNN classifica instâncias com base na proximidade com exemplos vizinhos. É sensível à escala dos dados (etapa já tratada no pré-processamento). Testaremos:\n",
    "\n",
    "* **N Neighbors (k):** `3`, `5`, `7`, `9`, `11`, `15` (número de vizinhos considerados).\n",
    "* **Weights:** `uniform` (todos têm peso igual) vs `distance` (vizinhos mais próximos têm maior influência na votação).\n",
    "* **Metric:** `euclidean` (distância padrão) vs `manhattan` (soma das diferenças absolutas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b7a7e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando o GridSearch para KNN ---\n",
      "Fitting 10 folds for each of 24 candidates, totalling 240 fits\n",
      "Melhor F1 para KNN: 0.9077\n",
      "Resultados salvos em: c:\\Users\\ga618\\.vscode\\Arquivos\\InteligenciaArtificial\\Projeto\\MachineLearnig-Classifiacacao\\results\\metrics\\KNN_results.csv\n",
      "Melhores parâmetros KNN: {'metric': 'manhattan', 'n_neighbors': 7, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "knn_params = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11, 15],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "knn_grid = run_classification_experiment(knn_model, knn_params, X, y, 'KNN')\n",
    "\n",
    "print(f\"Melhores parâmetros KNN: {knn_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac1144d",
   "metadata": {},
   "source": [
    "## 3. Naive Bayes\n",
    "\n",
    "O algoritmo Naive Bayes é um classificador probabilístico fundamentado no Teorema de Bayes, assumindo uma forte independência \"ingênua\" entre os atributos. Para dados contínuos, como neste dataset, utilizamos a variante Gaussiana e testaremos o seguinte parâmetro:\n",
    "\n",
    "* **Var Smoothing:** Parâmetro de estabilidade para cálculo de probabilidades (adiciona uma pequena variância para evitar erros de divisão por zero ou probabilidades nulas).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052970e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando o GridSearch para NaiveBayes ---\n",
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n",
      "Melhor F1 para NaiveBayes: 0.8299\n",
      "Resultados salvos em: c:\\Users\\ga618\\.vscode\\Arquivos\\InteligenciaArtificial\\Projeto\\MachineLearnig-Classifiacacao\\results\\metrics\\NaiveBayes_results.csv\n",
      "Melhores parâmetros NB: {'var_smoothing': 1e-09}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "nb_params = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6]\n",
    "}\n",
    "\n",
    "nb_grid = run_classification_experiment(nb_model, nb_params, X, y, 'NaiveBayes')\n",
    "\n",
    "print(f\"Melhores parâmetros NB: {nb_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c56c0d8",
   "metadata": {},
   "source": [
    "## 4. Regressão Logística (Logistic Regression)\n",
    "\n",
    "A Regressão Logística é usada para classificar dados calculando a probabilidade disso acontecer. Para melhorar o resultado, ajustamos a regularização simplificando o modelo:\n",
    "\n",
    "* **C:** Inverso da força de regularização (valores menores indicam regularização mais forte, prevenindo *overfitting*).\n",
    "* **Penalty:** \n",
    "\n",
    "`l1` (Lasso) :Pode zerar o peso de características inúteis.\n",
    "\n",
    "`l2` (Ridge) Apenas reduz o peso das características menos importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3520e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando o GridSearch para LogisticRegression ---\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ga618\\.vscode\\Arquivos\\InteligenciaArtificial\\Projeto\\MachineLearnig-Classifiacacao\\myfirstproject\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor F1 para LogisticRegression: 0.8849\n",
      "Resultados salvos em: c:\\Users\\ga618\\.vscode\\Arquivos\\InteligenciaArtificial\\Projeto\\MachineLearnig-Classifiacacao\\results\\metrics\\LogisticRegression_results.csv\n",
      "Melhores parâmetros LR: {'C': 10.0, 'penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression(random_state=42, solver='liblinear')\n",
    "\n",
    "lr_params = {\n",
    "    # 'C' é o inverso da força de regularização. Valores menores significam regularização mais forte.\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    # 'penalty' define o tipo de regularização: l1 ou l2. \n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "lr_grid = run_classification_experiment(lr_model, lr_params, X, y, 'LogisticRegression')\n",
    "\n",
    "print(f\"Melhores parâmetros LR: {lr_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58cc87f",
   "metadata": {},
   "source": [
    "## 5. Multi-layer Perceptron (MLP)\n",
    "\n",
    "O MLP é uma rede neural artificial *feedforward* capaz de capturar relações não-lineares complexas nos dados através de múltiplas camadas de neurônios. O ajuste de sua arquitetura e regularização é essencial:\n",
    "\n",
    "* **Hidden Layer Sizes:** Define a topologia da rede (quantidade de camadas ocultas e número de neurônios em cada uma).\n",
    "* **Activation:** `tanh` vs `relu` (função de ativação não-linear aplicada aos neurônios das camadas ocultas).\n",
    "* **Alpha:** Parâmetro de penalidade L2 para controlar a magnitude dos pesos e evitar *overfitting*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9459533c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando o GridSearch para MLP ---\n",
      "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n",
      "Melhor F1 para MLP: 0.8911\n",
      "Resultados salvos em: c:\\Users\\ga618\\.vscode\\Arquivos\\InteligenciaArtificial\\Projeto\\MachineLearnig-Classifiacacao\\results\\metrics\\MLP_results.csv\n",
      "Melhores parâmetros MLP: {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (100,)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_model = MLPClassifier(random_state=42, max_iter=500)\n",
    "\n",
    "mlp_params = {\n",
    "    'hidden_layer_sizes': [(50,), (50, 50), (100,)],\n",
    "\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    \n",
    "    'alpha': [0.0001, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "mlp_grid = run_classification_experiment(mlp_model, mlp_params, X, y, 'MLP')\n",
    "\n",
    "print(f\"Melhores parâmetros MLP: {mlp_grid.best_params_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myfirstproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
